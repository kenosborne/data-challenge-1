---
title: "Classification"
author: "Kenneth L Osborne"
date: "August 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(psych))
```

# Classification

Classification

You have explored and analyzed customer data collected by the Adventure Works Cycles company. Now you should be ready to apply what you have learned about the data to building, testing, and optimizing a predictive machine learning model.

Specifically, you must use any combination of Azure Machine Learning, R or Python to create a classification model that predicts whether or not a new customer will buy a bike.

Challenge Instructions
To complete this challenge:

1. Use the Adventure Works Cycles customer data you worked with in challenge 1 to create a classification model that predicts whether or not a customer will purchase a bike. The model should predict bike purchasing for new customers for whom no information about average monthly spend or previous bike purchases is available.
2. Download the test data. This data includes customer features but does not include bike purchasing or average monthly spend values.
3. Use your model to predict the corresponding test dataset. Don't forget to apply what you've learned throughout this course.
4. Go to the next page to check how well your prediction against the actual result.

## Data Prep

Loading the csv files into dataframes in R goes as follows. Note that the `customerData` dataset has been cleaned and explored in the previous section of the data challenge.
```{r}
testData <- read.csv("AW_test.csv", stringsAsFactors = FALSE)
customerData <- read.csv("../customerData.csv", stringsAsFactors = FALSE)
```

Looking at the structure of the new dataset:
```{r}
testData %>% str()
testData$CustomerID %>% unique() %>% length()
```

We have 500 observations of 23 different variables, each of which is unique. For now, that is all we need to know about the test set, that the data is "well behaved". We'll shift focus back to our training data for the present.

### Label Imbalance

With any label data, it's useful to know the label balance. For `BikeBuyer`, our current label of interest, we want to know how many people did vs did not buy bikes. We'll transform the label data to make it a bit easier to read, and then check the label balance. We'll do the same for the `HomeOwnerFlag` variable while we're at it.

```{r}
customerData %<>% 
  mutate(
    BikeBuyer = ifelse(BikeBuyer == 1, "yes", "no") %>% as.factor(),
    HomeOwnerFlag = ifelse(HomeOwnerFlag == 1, "yes", "no") %>% as.factor()
  )
table(customerData$BikeBuyer)
table(customerData$BikeBuyer)[1]/table(customerData$BikeBuyer)[2]
```

As we can see, the ratio of customers that didn't buy bikes to those who did is 2:1. We'll take that into consideration later. 

### Feature Selection

Next we'll go through the data and sort out which of our variables we'll include in our model. We'll also scale and center our numerical data.

```{r}
colnames(customerData)
```

Variables 1-13 just give basic identification information about the customers, and are not likely to correlate well with any predictive information. We'll drop them. 

The `BirthDate` variable is likely just a poor man's version of the `Age` or `Age Category` variable, as it is unlikely that bike buyers all have the commonalities in birth month or date. The `Age Category` variable is also dubious, as age categories were created as per the dictates of a data challenge question. The age category separations might be meaningful, but will need to be inspected further before we can just accept them as being more useful than just `Age`. As such, we'll simply drop it for now. 

On the other hand, the created variables of `CarCategory` and `ChildrenHome` did seem to split the data well. We'll keep these variables, and transform the `testing` data set to include them.

Finally, our testing data doesn't have access to the `AveMonthSpend` variable, and so we can drop it for the time being.


```{r}
customerData <- customerData %>% select(-(1:13),-BirthDate, -AgeCategory, -AveMonthSpend)
customerData %>% head(10) %>% as.tibble()
```

We'll scale and center the numeric variables.

```{r}
num_cols <- sapply(customerData,class) %in% c("integer","numeric") %>% 
  colnames(customerData)[.]

num_cols
```

We see that despite being numeric, the first three variables, ```NumberCarsOwned, NumberChildrenAtHome, TotalChildren``` are better classified as categorical variables than numerical. We correct this.

```{r}
fact_cols <- num_cols[1:3]
customerData[,fact_cols] %<>% sapply(., as.factor)
num_cols %<>% .[-(1:3)] #leave out the first three variables, they're more categorical
```

Then we check that our numerical data are well behaved.

```{r}
sapply(customerData[,num_cols], skew)
multi.hist(customerData[,num_cols])
```

Seeing that these data aren't crazily skewed, we can continue with the analysis

```{r}
preProcValues <- preProcess(customerData[,num_cols], method = c("center", "scale"))

ppCustomerData <- customerData
ppCustomerData[,num_cols] <-  predict(preProcValues, customerData[,num_cols])
head(ppCustomerData[,num_cols])
```

We check for low variance features that could be removed from the model.

```{r}
dummies <- dummyVars(data = ppCustomerData, BikeBuyer ~ .)
ppCustomerData_dummies <- data.frame(predict(dummies, newdata = ppCustomerData))

cat(
  "Column names:\n", colnames(ppCustomerData_dummies) %>% paste0(collapse = ", "), 
  "\n\nDimensions: ", dim(ppCustomerData_dummies) 
)
```

We'll check for low variance features. For our purposes, we'll define "low variance" to mean that 95% or more of the cases have the same value. (This parameter is used in `freqCut`)

```{r}
near_zero <-  nearZeroVar(ppCustomerData_dummies, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
low_variance_cols <- near_zero[(near_zero$zeroVar == TRUE) | (near_zero$nzv == TRUE), ]
low_variance_cols %>% as.tibble()
```

We have no low variance features in this dataset. We would normally remove (or at least be skeptical of) these low variance features, but there's nothing to do here.

In later rounds of revision, we might do Principle Component Analysis here, trying to find a way to best explain the data with the fewest number of variables, but for our first pass we'll used all the variables we're given. 

### Split Data

We'll split `customerData` into training and validation sets.

```{r}
partition  <- createDataPartition(ppCustomerData$BikeBuyer, times = 1, p = 0.7, list = FALSE)
training   <- customerData[ partition,] # Create the training sample
validation <- customerData[-partition,] # Create the test sample

trainingLabel   <- customerData[ partition,] %>% select(BikeBuyer)
validationLabel <- customerData[-partition,] %>% select(BikeBuyer)

cat(
  "Dim Training:  ", dim(training),
  "\nDim Validation: ", dim(validation)
)
```

## Determine Hyperparamets Using Nested CV

Before continuing we'll convert the remaining character features into factors.

```{r}
char_cols <- training %>% sapply(class) %in% "character"
training[,char_cols] %<>% lapply(., as.factor)

fact_cols <- training %>% sapply(class) %in% "factor"
training %>% head() %>% as.tibble
```

### Inner Loop CV

Because we want our data to be balanced, we weight the cases of bike buyers twice as heavily as non-bike buyers.

We're using the `trainControl` function to specify that we're doing cross validation (CV) across 10 folds, that we're going to record the probability of each class (along with predicted values) in each resample. The `twoClassSummary` function defines the metric as the used for model selection to be ROC.

The `train` function specifies that we're predicting BikeBuyer status based on all the other factors. That we're using the l2 norm "glmnet", how much to weight each case, that we're using the ROC metric, and that the CV information can be found in the `fitControl` object.

```{r}
## Create a weight vector for the training cases.
weights <- ifelse(trainingLabel == 'yes', 2/3, 1/3)

fitControl <- trainControl(
  method = 'cv',
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

cv_mod_roc <- train(
  BikeBuyer ~ .,
  data      = training, 
  method    = "glmnet", 
  weights   = weights, 
  metric    = "ROC",
  trControl = fitControl
)

cv_mod_roc
```

We can plot the results of hyperparameter search for maximal ROC 

```{r}
plot(
  cv_mod_roc, metric = "ROC", plotType = "level",
  scales = list(x = list(rot = 90))
)
```


With the inner loop complete we can examine an outer loop. Consistent performance across the folds indicates that the model is likely to generalize well when faced with new data values.

```{r}
## Set the hyperparameter grid to the optimal values from the inside loop
paramGrid <- expand.grid(alpha = c(cv_mod_roc$bestTune$alpha),
                         lambda = c(cv_mod_roc$bestTune$lambda))

fitControl = trainControl(method = 'cv',
                          number = 10,
                          returnResamp="all",
                          savePredictions = TRUE,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary)

set.seed(1234)
cv_mod_outer <- train(
  BikeBuyer ~ .,
  data      = training, 
  method    = "glmnet", 
  weights   = weights, 
  tuneGrid = paramGrid,
  metric    = "ROC",
  trControl = fitControl
)



print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), alpha = mod$resample[1,4], 
                lambda = mod$resample[1,5], Resample = 'Mean')
    stds = c(apply(mod$resample[,1:3], 2, sd), alpha = mod$resample[1,4], 
                lambda = mod$resample[1,5], Resample = 'STD')
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(cv_mod_outer)
```

As can be seen in the summary of our folds above with respect to the metrics of ROC, Sensitivity, and Specificity, the mean is an order of magnitude larger than the Standard Deviation. We also see that none of the folds has a performance metric values that look crazy compared to the mean value.

These observations are good indications that this model is likely to generalize well, as variation within performance metrics is limited.





## Classify our data

Let's take the testing data and apply our transformations to it so as to get 