---
title: "Classification"
author: "Kenneth L Osborne"
date: "August 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(psych))
```

# Classification

Classification

You have explored and analyzed customer data collected by the Adventure Works Cycles company. Now you should be ready to apply what you have learned about the data to building, testing, and optimizing a predictive machine learning model.

Specifically, you must use any combination of Azure Machine Learning, R or Python to create a classification model that predicts whether or not a new customer will buy a bike.

Challenge Instructions
To complete this challenge:

1. Use the Adventure Works Cycles customer data you worked with in challenge 1 to create a classification model that predicts whether or not a customer will purchase a bike. The model should predict bike purchasing for new customers for whom no information about average monthly spend or previous bike purchases is available.
2. Download the test data. This data includes customer features but does not include bike purchasing or average monthly spend values.
3. Use your model to predict the corresponding test dataset. Don't forget to apply what you've learned throughout this course.
4. Go to the next page to check how well your prediction against the actual result.

## Data Prep

Loading the csv files into dataframes in R goes as follows. Note that the `customerData` dataset has been cleaned and explored in the previous section of the data challenge.
```{r}
testData <- read.csv("AW_test.csv", stringsAsFactors = FALSE)
customerData <- read.csv("../customerData.csv", stringsAsFactors = FALSE)
```

Looking at the structure of the new dataset:
```{r}
testData %>% str()
testData$CustomerID %>% unique() %>% length()
```

We have 500 observations of 23 different variables, each of which is unique. For now, that is all we need to know about the test set, that the data is "well behaved". We'll shift focus back to our training data for the present.

### Label Imbalance

With any label data, it's useful to know the label balance. For `BikeBuyer`, our current label of interest, we want to know how many people did vs did not buy bikes. We'll transform the label data to make it a bit easier to read, and then check the label balance. We'll do the same for the `HomeOwnerFlag` variable while we're at it.

```{r}
customerData %<>% 
  mutate(
    BikeBuyer = ifelse(BikeBuyer == 1, "yes", "no") %>% as.factor(),
    HomeOwnerFlag = ifelse(HomeOwnerFlag == 1, "yes", "no") %>% as.factor()
  )
table(customerData$BikeBuyer)
table(customerData$BikeBuyer)[1]/table(customerData$BikeBuyer)[2]
```

As we can see, the ratio of customers that didn't buy bikes to those who did is 2:1. We'll take that into consideration later. 

### Feature Selection

Next we'll go through the data and sort out which of our variables we'll include in our model. We'll also scale and center our numerical data.

```{r}
colnames(customerData)
```

Variables 1-13 just give basic identification information about the customers, and are not likely to correlate well with any predictive information. We'll drop them. 

The `BirthDate` variable is likely just a poor man's version of the `Age` or `Age Category` variable, as it is unlikely that bike buyers all have the commonalities in birth month or date. The `Age Category` variable is also dubious, as age categories were created as per the dictates of a data challenge question. The age category separations might be meaningful, but will need to be inspected further before we can just accept them as being more useful than just `Age`. As such, we'll simply drop it for now. 

On the other hand, the created variables of `CarCategory` and `ChildrenHome` did seem to split the data well. We'll keep these variables, and transform the `testing` data set to include them.

Finally, our testing data doesn't have access to the `AveMonthSpend` variable, and so we can drop it for the time being.


```{r}
customerData <- customerData %>% select(-(1:13),-BirthDate, -AgeCategory, -AveMonthSpend)
customerData %>% head(10) %>% as.tibble()
```

We'll scale and center the numeric variables.

```{r}
num_cols <- sapply(customerData,class) %in% c("integer","numeric") %>% 
  colnames(customerData)[.]

num_cols
```

We see that despite being numeric, the first three variables, ```NumberCarsOwned, NumberChildrenAtHome, TotalChildren``` are better classified as categorical variables than numerical. We correct this.

```{r}
fact_cols <- num_cols[1:3]
customerData[,fact_cols] %<>% sapply(., as.factor)
num_cols %<>% .[-(1:3)] #leave out the first three variables, they're more categorical
```

Then we check that our numerical data are well behaved.

```{r}
sapply(customerData[,num_cols], skew)
multi.hist(customerData[,num_cols])
```

Seeing that these data aren't crazily skewed, we can continue with the analysis

```{r}
preProcValues <- preProcess(customerData[,num_cols], method = c("center", "scale"))

ppCustomerData <- customerData
ppCustomerData[,num_cols] <-  predict(preProcValues, customerData[,num_cols])
head(ppCustomerData[,num_cols])
```

We check for low variance features that could be removed from the model.

```{r}
dummies <- dummyVars(data = ppCustomerData, BikeBuyer ~ .)
ppCustomerData_dummies <- data.frame(predict(dummies, newdata = ppCustomerData))

cat(
  "Column names:\n", colnames(ppCustomerData_dummies) %>% paste0(collapse = ", "), 
  "\n\nDimensions: ", dim(ppCustomerData_dummies) 
)
```

We'll check for low variance features. For our purposes, we'll define "low variance" to mean that 95% or more of the cases have the same value. (This parameter is used in `freqCut`)

```{r}
near_zero <-  nearZeroVar(ppCustomerData_dummies, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
low_variance_cols <- near_zero[(near_zero$zeroVar == TRUE) | (near_zero$nzv == TRUE), ]
low_variance_cols %>% as.tibble()
```

We have no low variance features in this dataset. We would normally remove (or at least be skeptical of) these low variance features, but there's nothing to do here.

In later rounds of revision, we might do Principle Component Analysis here, trying to find a way to best explain the data with the fewest number of variables, but for our first pass we'll used all the variables we're given. 

### Split Data

We'll split `customerData` into training and validation sets.

```{r}
partition  <- createDataPartition(ppCustomerData$BikeBuyer, times = 1, p = 0.7, list = FALSE)
training   <- customerData[ partition,] # Create the training sample
validation <- customerData[-partition,] # Create the test sample

trainingLabel   <- customerData[ partition,] %>% select(BikeBuyer)
validationLabel <- customerData[-partition,] %>% select(BikeBuyer)

cat(
  "Dim Training:  ", dim(training),
  "\nDim Validation: ", dim(validation)
)
```

## Determine Hyperparamets Using Nested CV

Before continuing we'll convert the remaining character features into factors.

```{r}
char_cols <- training %>% sapply(class) %in% "character"
training[,char_cols] %<>% lapply(., as.factor)

fact_cols <- training %>% sapply(class) %in% "factor"
training %>% head() %>% as.tibble
```

### Inner Loop CV

Because we want our data to be balanced, we weight the cases of bike buyers twice as heavily as non-bike buyers.

We're using the `trainControl` function to specify that we're doing cross validation (CV) across 10 folds, that we're going to record the probability of each class (along with predicted values) in each resample. The `twoClassSummary` function defines the metric as the used for model selection to be ROC.

The `train` function specifies that we're predicting BikeBuyer status based on all the other factors. That we're using the l2 norm "glmnet", how much to weight each case, that we're using the ROC metric, and that the CV information can be found in the `fitControl` object.

```{r}
## Create a weight vector for the training cases.
weights <- ifelse(trainingLabel == 'yes', 2/3, 1/3)

fitControl <- trainControl(
  method = 'cv',
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

cv_mod_roc <- train(
  BikeBuyer ~ .,
  data      = training, 
  method    = "glmnet", 
  weights   = weights, 
  metric    = "ROC",
  trControl = fitControl
)

cv_mod_roc
```

We can plot the results of hyperparameter search for maximal ROC 

```{r}
plot(
  cv_mod_roc, metric = "ROC", plotType = "level",
  scales = list(x = list(rot = 90))
)
```


With the inner loop complete we can examine an outer loop. Consistent performance across the folds indicates that the model is likely to generalize well when faced with new data values.

```{r}
## Set the hyperparameter grid to the optimal values from the inside loop
paramGrid <- expand.grid(alpha = c(cv_mod_roc$bestTune$alpha),
                         lambda = c(cv_mod_roc$bestTune$lambda))

fitControl = trainControl(method = 'cv',
                          number = 10,
                          returnResamp="all",
                          savePredictions = TRUE,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary)

set.seed(1234)
cv_mod_outer <- train(
  BikeBuyer ~ .,
  data      = training, 
  method    = "glmnet", 
  weights   = weights, 
  tuneGrid = paramGrid,
  metric    = "ROC",
  trControl = fitControl
)



print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), alpha = mod$resample[1,4], 
                lambda = mod$resample[1,5], Resample = 'Mean')
    stds = c(apply(mod$resample[,1:3], 2, sd), alpha = mod$resample[1,4], 
                lambda = mod$resample[1,5], Resample = 'STD')
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(cv_mod_outer)
```

As can be seen in the summary of our folds above with respect to the metrics of ROC, Sensitivity, and Specificity, the mean is an order of magnitude larger than the Standard Deviation. We also see that none of the folds has a performance metric values that look crazy compared to the mean value.

Furthermore, the same values of alpha and lambda were selected as optimal for all folds. We'll make a ROC plot using the optimal values.

```{r}
alphaOptimal  <- cv_mod_outer$resample$alpha[ 1] #using the first value because they're all the same
lambdaOptimal <- cv_mod_outer$resample$lambda[1] #using the first value because they're all the same

roc_obj <- roc(
  
  cv_mod_outer$pred %>% 
    filter(alpha == alphaOptimal, lambda == lambdaOptimal) %>%
    .[,"obs"], 
  
  cv_mod_outer$pred %>% 
    filter(alpha == alphaOptimal, lambda == lambdaOptimal) %>%
    .[,"yes"]
  
)

ggroc(roc_obj, color = "red") + 
  geom_abline(intercept = 1, slope = 1) + 
  geom_hline(yintercept = 0, size=1.3) +
  geom_vline(xintercept = 1, size=1.3) +
  theme_minimal()
```


These observations are good indications that this model is likely to generalize well, as variation within performance metrics is limited. On the other hand, with ROC hovering at 84%, it is likely we'll misclassify bike buyers.





### Classify our data

Let's take the validation data and see how well our model predicts bike buyers.

```{r}
validation_prpedictions <- predict(cv_mod_outer, validation)
head(validation_prpedictions)
```

And then we'll see how our predictions align with reality
```{r}
confusionMatrix(validationLabel[,1], validation_prpedictions, positive = "yes")
```

As expected, the accuracy of our model is limited. One way to improve model accuracy would be to go back and do PCA, or by simply eliminating variable that are less useful in our model. Other ways to improve the model would be to run a search on more values of the parameters `alpha` and `lambda`. But for the sake of demonstrating a different skill set, I'm going to do a second analysis of the same data using a neural network.

## Prediction with Neural Nets

A neural net takes in our variables, and then tries to build a function that predicts the outputs. In general, we could conceive of having a neural network with many layers; however, in practice (especially with most practical data science and ML), we only choose to operate with 1 to 2 layers. More layers add complexity, but at the cost of training time, and a lack of certainty as to whether or not the model has converged / will converge.

This example will include a neural net with one hidden layer. The number of nodes contained in that hidden layer we'll let the model work out for itself.

```{r cachedChunk, cache=TRUE}
fitControl <- trainControl(
  method = "cv",
  number = 5,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

paramGrid <- expand.grid(size = c(3, 6, 9, 15), decay = c(10, 1, 0.1, 0.01))

set.seed(1234)
nn_fit_inside_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "nnet", # Neural network model 
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  trace = FALSE,
  metric="ROC")

print(nn_fit_inside_tw)
```

After optimizing for ROC, we get the optimal decay and network size. Given this new model, which features of the data set were most important?

```{r}
options(repr.plot.width=8, repr.plot.height=6)
var_imp_nn <- varImp(nn_fit_inside_tw)
print(var_imp_nn)
plot(var_imp_nn)
```

Delving a bit further, we can map out how our parameter sweep of decay and size effect the overall ROC.

```{r}
trellis.par.set(caretTheme())
plot(nn_fit_inside_tw)
```

By the looks of things, it seems that decay values of 0.01 and 10 underperform compared to the more moderate values of 0.1 and 1. This indicates that we're nearing an optimal value with the middle two parameters, although with only 16 data points it's hard to say conclusively. Furthermore, of the number of nodes values we selected, around 9 performed best.

To verify that the model will generalize well we perform an outer cv loop. 

```{r}
fitControl <- trainControl(
  method = "cv",
  number = 5,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary)

paramGrid <- expand.grid(size  = c(nn_fit_inside_tw$bestTune$size[1]), 
                         decay = c(nn_fit_inside_tw$bestTune$decay[1]))

set.seed(1234)
nn_fit_outer_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "nnet",
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  trace = FALSE,
  metric="ROC")

print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), 
              size = mod$resample[1,4], 
              decay = mod$resample[1,5], 
              Resample = 'Mean')
    
    stds = c(apply(mod$resample[,1:3], 2, sd), 
             size = mod$resample[1,4], 
             decay = mod$resample[1,5], 
             Resample = 'STD')
    
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(nn_fit_outer_tw)
```

The ROC mean and standard deviation are two orders of magnitude different from one another. It's interesting that both the neural network method and a simple cross validation yield similar ROC values.

### Cross-correlating Results

It would be interesting to see how our two models compare with one another.

If they differ in predictions, it might be possible we can make an ensemble of predictions that proves more accurate than any one model. For this case, the neural network model is the "prediction" class and the cv model is the "reference" class. 

```{r}
nn_training_predictions <- predict(nn_fit_outer_tw, training)
cv_training_predictions <- predict(cv_mod_outer,    training)

confusionMatrix(nn_training_predictions, cv_training_predictions, positive = "yes")
```

Let's see how well the neural net performs with respect to the validation label data.

```{r}
nn_validation_pred <- predict(nn_fit_outer_tw, validation)
cv_validation_pred <- predict(cv_mod_outer,    validation)

confusionMatrix(nn_validation_pred, validationLabel[,1], positive = "yes")
```

We see that the neural network is performing at around a 77% correct prediction rate, similar to the cross CV model. But how often do these models make the same predictions?

```{r}
confusionMatrix(nn_validation_pred, cv_validation_pred, positive = "yes")
```

The vast majority of the time (>95%) when one model would predict "yes" for `BikeBuyer`, the other model would as well. This means that averaging the numerical predictions is unlikely to significantly change our model accuracy (at best around 5%).

We could try to better tune the model, searching for better hyperparameters, or we could create / remove model features to try to improve results, but since we're doing a survey of different techniques, let's try something completely different.

## Random Forests

Being that neither of these models is amazing, we'll attempt a third model using random forests.


```{r}
fitControl <- trainControl(
  method = "cv",
  number = 5,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

paramGrid <- expand.grid(mtry = c(5, 10, 15))

set.seed(1234)
rf_fit_inside_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "rf", # Random forest model 
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  metric = "ROC"
)

print(rf_fit_inside_tw)
```


```{r}
options(repr.plot.width=8, repr.plot.height=6)
var_imp_tree <- varImp(rf_fit_inside_tw)
print(var_imp_tree)
plot(var_imp_tree)
```

```{r}
tr_validation_prediction <- predict()
```

