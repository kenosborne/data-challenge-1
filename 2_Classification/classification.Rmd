---
title: "Classification"
author: "Kenneth L Osborne"
date: "August 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(caretEnsemble))
suppressPackageStartupMessages(library(mlbench))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(psych))
```

# Classification

Classification

You have explored and analyzed customer data collected by the Adventure Works Cycles company. Now you should be ready to apply what you have learned about the data to building, testing, and optimizing a predictive machine learning model.

Specifically, you must use any combination of Azure Machine Learning, R or Python to create a classification model that predicts whether or not a new customer will buy a bike.

Challenge Instructions
To complete this challenge:

1. Use the Adventure Works Cycles customer data you worked with in challenge 1 to create a classification model that predicts whether or not a customer will purchase a bike. The model should predict bike purchasing for new customers for whom no information about average monthly spend or previous bike purchases is available.
2. Download the test data. This data includes customer features but does not include bike purchasing or average monthly spend values.
3. Use your model to predict the corresponding test dataset. Don't forget to apply what you've learned throughout this course.
4. Go to the next page to check how well your prediction against the actual result.

## Data Prep

Loading the csv files into dataframes in R goes as follows. Note that the `customerData` dataset has been cleaned and explored in the previous section of the data challenge.
```{r}
customerData <- read.csv("../customerData.csv", stringsAsFactors = FALSE)
```

This is the same data that was cleaned with the data exploration.

### Label Imbalance

With any label data, it's useful to know the label balance. For `BikeBuyer`, our current label of interest, we want to know how many people did vs did not buy bikes. We'll transform the label data to make it a bit easier to read, and then check the label balance. We'll do the same for the `HomeOwnerFlag` variable while we're at it.

```{r}
customerData %<>% 
  mutate(
    BikeBuyer = ifelse(BikeBuyer == 1, "yes", "no") %>% as.factor(),
    HomeOwnerFlag = ifelse(HomeOwnerFlag == 1, "yes", "no") %>% as.factor()
  )
table(customerData$BikeBuyer)

ratio <- table(customerData$BikeBuyer)[1]/table(customerData$BikeBuyer)[2]
ratio
```

As we can see, the ratio of customers that didn't buy bikes to those who did is 2:1. We'll take that into consideration later. 

### Feature Selection

Next we'll go through the data and sort out which of our variables we'll include in our model. We'll also scale and center our numerical data.

```{r}
colnames(customerData)
```

Variables 1-13 just give basic identification information about the customers, and are not likely to correlate well with any predictive information. We'll drop them. 

The `BirthDate` variable is likely just a poor man's version of the `Age` or `Age Category` variable, as it is unlikely that bike buyers all have the commonalities in birth month or date. The `Age Category` variable is also dubious, as age categories were created as per the dictates of a data challenge question. The age category separations might be meaningful, but will need to be inspected further before we can just accept them as being more useful than just `Age`. As such, we'll simply drop it for now. 

On the other hand, the created variables of `CarCategory` and `ChildrenHome` did seem to split the data well. We'll keep these variables, and transform the `testing` data set to include them.

Finally, our testing data doesn't have access to the `AveMonthSpend` variable, and so we can drop it for the time being.


```{r}
customerData <- customerData %>% select(-(1:13),-BirthDate, -AgeCategory, -AveMonthSpend)
customerData %>% head(10) %>% as.tibble()
```

We'll scale and center the numeric variables.

```{r}
num_cols <- sapply(customerData,class) %in% c("integer","numeric") %>% 
  colnames(customerData)[.]

num_cols
```

We see that despite being numeric, the first three variables, `NumberCarsOwned, NumberChildrenAtHome, TotalChildren` are better classified as categorical variables than numerical. We correct this.

```{r}
fact_cols <- num_cols[1:3]
customerData[,fact_cols] %<>% sapply(., as.factor)
num_cols %<>% .[-(1:3)] #leave out the first three variables, they're more categorical
```

Then we check that our numerical data are well behaved.

```{r}
sapply(customerData[,num_cols], skew)
multi.hist(customerData[,num_cols])
```

Seeing that these data aren't crazily skewed, we can continue with the analysis

```{r}
preProcValues <- preProcess(customerData[,num_cols], method = c("center", "scale"))

ppCustomerData <- customerData
ppCustomerData[,num_cols] <-  predict(preProcValues, customerData[,num_cols])
head(ppCustomerData[,num_cols])
```

Before continuing we'll convert the remaining character features into factors.

```{r}
char_cols <- ppCustomerData %>% sapply(class) %in% "character"
ppCustomerData[,char_cols] %<>% lapply(., as.factor)

fact_cols <- ppCustomerData %>% sapply(class) %in% "factor"
ppCustomerData %>% head() %>% as.tibble
```

We check for low variance features that could be removed from the model.

```{r}
dummies <- dummyVars(data = ppCustomerData, BikeBuyer ~ .)
ppCustomerData_dummies <- data.frame(predict(dummies, newdata = ppCustomerData))

cat(
  "Column names:\n", colnames(ppCustomerData_dummies) %>% paste0(collapse = ", "), 
  "\n\nDimensions: ", dim(ppCustomerData_dummies) 
)
```

We'll check for low variance features. For our purposes, we'll define "low variance" to mean that 95% or more of the cases have the same value. (This parameter is used in `freqCut`)

```{r}
near_zero <-  nearZeroVar(ppCustomerData_dummies, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
low_variance_cols <- near_zero[(near_zero$zeroVar == TRUE) | (near_zero$nzv == TRUE), ]
low_variance_cols %>% as.tibble()
```

We have no low variance features in this dataset. We would normally remove (or at least be skeptical of) these low variance features, but there's nothing to do here.

In later rounds of revision, we might do Principle Component Analysis here, trying to find a way to best explain the data with the fewest number of variables, but for our first pass we'll used all the variables we're given. 

### Split Data

We'll split `customerData` into training and validation sets. Normally the data is split into training and testing data sets, but since this data challenge was connected to an online class that wanted me to make predictions with a different set of "testing" data, I split the data like this.

```{r}
partition  <- createDataPartition(ppCustomerData$BikeBuyer, times = 1, p = 0.7, list = FALSE)
training   <- customerData[ partition,] # Create the training sample
validation <- customerData[-partition,] # Create the test sample

trainingLabel   <- customerData[ partition,] %>% select(BikeBuyer)
validationLabel <- customerData[-partition,] %>% select(BikeBuyer)

cat(
  "Dim Training:  ", dim(training),
  "\nDim Validation: ", dim(validation)
)
```

## Determine Hyperparameters Using Nested CV

This method looks at how well hyperparameters perform across different folds, and then averages them.

### Inner Loop CV

Because we want our data to be balanced, we weight the cases of bike buyers twice as heavily as non-bike buyers.

We're using the `trainControl` function to specify that we're doing cross validation (CV) across 10 folds, that we're going to record the probability of each class (along with predicted values) in each resample. The `twoClassSummary` function defines the metric as the used for model selection to be ROC.

The `train` function specifies that we're predicting BikeBuyer status based on all the other factors. That we're using the a mixing of the L1 and L2 norm in the method "glmnet". We define weights for each case, that we're using the ROC metric, and that the CV information can be found in the `fitControl` object.

```{r cc-1, cache=TRUE}
## Create a weight vector for the training cases.
weights <- ifelse(trainingLabel == 'yes', 2/3, 1/3)

glmGrid <- expand.grid(
  alpha = c(10^(-3:-1), 3*10^(-3:-1), .7, 1),
  lambda = 10^(-5:-2)
)

fitControl <- trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

cv_mod_roc <- train(
  BikeBuyer ~ .,
  data      = training, 
  method    = "glmnet", 
  weights   = weights, 
  metric    = "ROC",
  trControl = fitControl,
  tuneGrid  = glmGrid
)

cv_mod_roc
```

We can plot the results of hyperparameter search for maximal ROC.

```{r}
ggplot(cv_mod_roc) + 
  scale_x_continuous(trans = 'log10') +
  theme_minimal()
```

This plot shows that our nested CV model is doing a good job optimizing: neither alpha  nor lambda get improve as they deviate from our optimal value. On the other hand, there are many values for alpha and lambda that are close to the optimal value. We'll just choose the best performer for right now.

A model can sometimes be improved by pruning unimportant features.
```{r}
var_imp_glm <- varImp(cv_mod_roc)
var_imp_glm$importance %>% 
  rownames_to_column() %>% 
  arrange(rowname)
plot(var_imp_glm)
```

In this case, `CarCategory` and `YearlyIncome` seem to provide no information for the model, so they would be better left out of the model going forward.

With the inner loop complete we can examine an outer loop. Consistent performance across the folds indicates that the model is likely to generalize well when faced with new data values.

```{r cachedChunk0, cache=TRUE}
## Set the hyperparameter grid to the optimal values from the inside loop
paramGrid <- expand.grid(alpha = c(cv_mod_roc$bestTune$alpha),
                         lambda = c(cv_mod_roc$bestTune$lambda))

fitControl = trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 3,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(1234)
cv_mod_outer <- train(
  BikeBuyer ~ .,
  data      = training %>% 
    select(-CarCategory, -YearlyIncome), 
  method    = "glmnet", 
  weights   = weights, 
  tuneGrid = paramGrid,
  metric    = "ROC",
  trControl = fitControl
)



print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), 
              alpha = mod$resample[1,4], 
              lambda = mod$resample[1,5], 
              Resample = 'Mean')
    
    stds = c(apply(mod$resample[,1:3], 2, sd), 
             alpha = mod$resample[1,4], 
             lambda = mod$resample[1,5], 
             Resample = 'STD')
    
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(cv_mod_outer)
```

As can be seen in the summary of our folds above with respect to the metrics of ROC, Sensitivity, and Specificity, the mean is an order of magnitude larger than the Standard Deviation. We also see that none of the folds has a performance metric values that look crazy compared to the mean value.

Furthermore, the same values of alpha and lambda were selected as optimal for all folds. We'll make a ROC plot using the optimal values.

```{r}
alphaOptimal  <- cv_mod_outer$resample$alpha[ 1] #using the first value because they're all the same
lambdaOptimal <- cv_mod_outer$resample$lambda[1] #using the first value because they're all the same

roc_obj <- roc(
  
  cv_mod_outer$pred %>% 
    filter(alpha == alphaOptimal, lambda == lambdaOptimal) %>%
    .[,"obs"], 
  
  cv_mod_outer$pred %>% 
    filter(alpha == alphaOptimal, lambda == lambdaOptimal) %>%
    .[,"yes"]
  
)

ggroc(roc_obj, color = "red") + 
  geom_abline(intercept = 1, slope = 1) + 
  geom_hline(yintercept = 0, size=1.3) +
  geom_vline(xintercept = 1, size=1.3) +
  theme_minimal()
```


These observations are good indications that this model is likely to generalize well, as variation within performance metrics is limited. On the other hand, with ROC hovering at 84%, it is likely we'll misclassify bike buyers.





### Classify our data

Let's take the validation data and see how well our model predicts bike buyers.

```{r}
validation_prpedictions <- predict(cv_mod_outer, validation)
head(validation_prpedictions)
```

And then we'll see how our predictions align with reality
```{r}
confusionMatrix(validationLabel[,1], validation_prpedictions, positive = "yes")
```

As expected, the accuracy of our model is limited. One way to improve model accuracy would be to go back and do PCA, or by simply eliminating variable that are less useful in our model. Other ways to improve the model would be to run a search on more values of the parameters `alpha` and `lambda`. But for the sake of demonstrating a different skill set, I'm going to do a second analysis of the same data using a neural network.

## Prediction with Neural Nets

A neural net takes in our variables, and then tries to build a function that predicts the outputs. In general, we could conceive of having a neural network with many layers; however, in practice (especially with most practical data science and ML), we only choose to operate with 1 to 2 layers. More layers add complexity, but at the cost of training time, and a lack of certainty as to whether or not the model has converged / will converge.

This example will include a neural net with one hidden layer. The number of nodes contained in that hidden layer we'll let the model work out for itself.

```{r cachedChunk1, cache=TRUE}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

paramGrid <- expand.grid(size = c(6, 9, 15, 20), decay = c(10, 1, 0.1, 0.01))

set.seed(1234)
nn_fit_inside_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "nnet", # Neural network model 
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  trace = FALSE,
  metric="ROC")

print(nn_fit_inside_tw)
```

After optimizing for ROC, we get the optimal decay and network size. Given this new model, which features of the data set were most important?

```{r}
options(repr.plot.width=8, repr.plot.height=6)
var_imp_nn <- varImp(nn_fit_inside_tw)
var_imp_nn$importance %>% 
  rownames_to_column() %>% 
  arrange(rowname)
plot(var_imp_nn)
```

We can none of the features were particularly not useful to the model. Delving a bit further, we can map out how our parameter sweep of decay and size effect the overall ROC.

```{r}
ggplot(nn_fit_inside_tw) + theme_minimal()
```

By the looks of things, it seems that decay values of 0.01 and 10 underperform compared to the more moderate values of 0.1 and 1. This indicates that we're nearing an optimal value with the middle two parameters, although with only 16 data points and no strong trend it's hard to say conclusively. Furthermore, of the number of nodes values we selected, 15 performed best.

To verify that the model will generalize well we perform an outer cv loop. Note that the model is removing some less-important features.

```{r cachedChunk2, cache=TRUE}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary)

paramGrid <- expand.grid(size  = c(nn_fit_inside_tw$bestTune$size[1]), 
                         decay = c(nn_fit_inside_tw$bestTune$decay[1]))

set.seed(1234)
nn_fit_outer_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "nnet",
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  trace = FALSE,
  metric="ROC")

print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), 
              size = mod$resample[1,4], 
              decay = mod$resample[1,5], 
              Resample = 'Mean')
    
    stds = c(apply(mod$resample[,1:3], 2, sd), 
             size = mod$resample[1,4], 
             decay = mod$resample[1,5], 
             Resample = 'STD')
    
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(nn_fit_outer_tw)
```

The ROC mean and standard deviation are two orders of magnitude different from one another. The inner CV and outer CV have approximately the same ROC, meaning that removing the features likely improved our model.

On another note, it's interesting that both the neural network method and a simple cross validation yield similar ROC values.

### Cross-correlating Results

It would be interesting to see how our two models compare with one another.

If they differ in predictions, it might be possible we can make an ensemble of predictions that proves more accurate than any one model. For this case, the neural network model is the "prediction" class and the cv model is the "reference" class. 

```{r}
nn_training_predictions <- predict(nn_fit_outer_tw, training)
cv_training_predictions <- predict(cv_mod_outer,    training)

confusionMatrix(nn_training_predictions, cv_training_predictions, positive = "yes")
```

Let's see how well the neural net performs with respect to the validation label data.

```{r}
nn_validation_pred <- predict(nn_fit_outer_tw, validation)
cv_validation_pred <- predict(cv_mod_outer,    validation)

confusionMatrix(nn_validation_pred, validationLabel[,1], positive = "yes")
```

We see that the neural network is performing at around a 75% correct prediction rate, similar to the cross CV model. But how often do these models make the same predictions about the validation data?

```{r}
confusionMatrix(nn_validation_pred, cv_validation_pred, positive = "yes")
```

The majority of the time (>90%) when one model would predict "yes" for `BikeBuyer`, the other model would as well. This means that stacking the numerical predictions is unlikely to significantly change our model accuracy (at best around 5%).

We could try to better tune the model, searching for better hyperparameters, or we could create / remove model features to try to improve results, but since we're doing a survey of different techniques, let's try something completely different.

## Random Forests

Being that neither of these models is amazing, we'll attempt a third model using random forests.


```{r cachedChunk3, cache=TRUE}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

paramGrid <- expand.grid(mtry = c(4:7))

set.seed(1234)
rf_fit_inside_tw <- train(
  BikeBuyer ~ ., 
  data = training,  
  method = "rf", # Random forest model 
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  metric = "ROC"
)

print(rf_fit_inside_tw)
```

Next we visualize how the parameter `mtry` correlated with ROC.
```{r}
ggplot(rf_fit_inside_tw) + theme_minimal()
```



```{r}
var_imp_tree <- varImp(rf_fit_inside_tw)
var_imp_tree$importance %>% 
  rownames_to_column %>% 
  arrange(rowname)
plot(var_imp_tree)
```

In this model it seems that features are playing at least some role in the model. although we could try removing `Education` and/or `Occupation` to see if our results improve, we'll leave all the features be for now. 

```{r cachedChunk4, cache=TRUE}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  returnResamp="all",
  savePredictions = TRUE,
  classProbs = TRUE,
  summaryFunction = twoClassSummary)

paramGrid <- expand.grid(mtry  = c(rf_fit_inside_tw$bestTune$mtry[1]))

set.seed(1234)
rf_fit_outer_tw <- train(
  BikeBuyer ~ ., 
  data = training,
  method = "rf",
  trControl = fitControl, 
  tuneGrid = paramGrid, 
  weights = weights, 
  trace = FALSE,
  metric="ROC")

print_metrics = function(mod){
    means = c(apply(mod$resample[,1:3], 2, mean), 
              mtry = mod$resample[1,4],
              Resample = 'Mean')
    
    stds = c(apply(mod$resample[,1:3], 2, sd), 
             mtry = mod$resample[1,4], 
             Resample = 'STD')
    
    out = rbind(mod$resample, means, stds)
    out[,1:3] = lapply(out[,1:3], function(x) round(as.numeric(x), 3))
    out
}

print_metrics(rf_fit_outer_tw)
```

```{r}
tr_validation_prediction <- predict(rf_fit_outer_tw, validation)
confusionMatrix(tr_validation_prediction, validationLabel[,1], positive = "yes")
```

The random forest method is performing significantly better than the glm or neural network models, with around 85% accuracy. 

## Stacking Algorithms

We can combine our models' predictions in a general linearized model, perhaps allowing the strengths of each model to shine through.

```{r}
predictionDF <- data.frame(
  validationLabel = validationLabel$BikeBuyer,
  cv_validation_pred,
  nn_validation_pred,
  tr_validation_prediction
)

modelStack <- train(validationLabel~., predictionDF, method = "glm")

print(modelStack)
```

We see that the stacked models perform better than any individual model. Furthermore here is the relative variable importance.

```{r}
var_imp_ms <- varImp(modelStack)
var_imp_ms$importance %>% 
  rownames_to_column() %>% 
  arrange(rowname)
plot(var_imp_ms)
```

The tree model made the most accurate predictions, while the glm and the neural net played less of a role in the final model.

Finally, we check to see how well the stacked model performed.

```{r}
ms_validation_pred <- predict(modelStack, validation)
confusionMatrix(ms_validation_pred, validationLabel[,1], positive  = "yes")
```

## Overview

Together the stacked model had a successful prediciton rate of around 85%, which not significantly different than the simple random forest model. It seems that the gains that the other models provided were offset by the losses of making incorrect predictions. Perhaps a different stacking method (e.g. random forest) would give better results, or perhaps the stacking method should have its own hyperparameter sweet to achieve optimal effectiveness.

If we had to choose one model right now, we would use a random forest model for predicting whether or not a person is likely to buy a bike from this shop. It will be computationally more efficient and simpler than using 3 stacked models, and is likely to be just as accurate in the end. 

We could also work to further improve the random forest model (and/or other models) by doing a principle component analysis beforehand, or by removing variables that have relatively low impact on our model. We could also change our inner cross validation parameters, altering the number of folds and or the number of repeats, in order to increase accuracy or reduce bias. As a final step, we would allow the model to do a final pass using the complete dataset (both training and validation) for our random trees. This could help improve the accuracy of a finalized model.

But for now we'll leave the models as they are, and point out what could be done differently if it were important to have better results.